{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Condition Classification Analysis\n",
    "\n",
    "This notebook analyzes the ability of different machine learning models to classify storage conditions of bakery products based on spectral data. We compare the performance of models with and without product type information to determine whether this additional feature improves classification accuracy.\n",
    "\n",
    "## Overview\n",
    "- **Data Source**: Spectral data from bakery products stored under different conditions\n",
    "- **Classification Target**: Storage conditions (Humid, Open, Wrapped)\n",
    "- **Models Evaluated**: SVM, Random Forest, KNN, Neural Network, Logistic Regression\n",
    "- **Key Metrics**: Accuracy, ROC AUC, Statistical Significance (t-test)\n",
    "\n",
    "The analysis builds on hyperparameter tuning results from previous experiments, which identified optimal model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "\n",
    "This section imports the necessary libraries and configures constants for the analysis. We import:\n",
    "- Data manipulation: pandas, numpy\n",
    "- Visualization: matplotlib, seaborn\n",
    "- Machine learning: scikit-learn models and evaluation metrics\n",
    "- Statistical analysis: scipy.stats\n",
    "\n",
    "Constants like random seed and cross-validation splits are set to ensure consistency with previous hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict, learning_curve\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
    "                            roc_curve, auc, roc_auc_score)\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Import models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Constants - matched with hyperparameter tuning\n",
    "RANDOM_STATE = 42\n",
    "CV_SPLITS = 5 # Consistent with hyperparameter tuning\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = 'results/Storage_Condition_Classification_results'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Filter warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Stochastic Optimizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Model Loading\n",
    "\n",
    "This section defines functions for:\n",
    "1. Loading and preprocessing the bakery spectral data\n",
    "2. Loading the best parameters identified from hyperparameter tuning\n",
    "3. Creating model instances with these optimized parameters\n",
    "\n",
    "The data preparation preserves the same preprocessing steps used during hyperparameter tuning to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Preparation and Model Loading Functions\n",
    "def prepare_data():\n",
    "    \"\"\"Load and prepare data consistently\"\"\"\n",
    "    print(\"Loading data and preparing features...\")\n",
    "    data = pd.read_csv('augmented_bakery_data.csv')\n",
    "    \n",
    "    # Encode labels\n",
    "    le_product = LabelEncoder()\n",
    "    le_storage = LabelEncoder()\n",
    "    data['Product_Type_encoded'] = le_product.fit_transform(data['Product_Type'])\n",
    "    data['Storage_Condition_encoded'] = le_storage.fit_transform(data['Storage_Condition'])\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_cols = [col for col in data.columns if col.startswith(('gain_', 'phase_'))]\n",
    "    X_base = data[feature_cols]\n",
    "    \n",
    "    # Scale features on entire dataset\n",
    "    scaler = StandardScaler()\n",
    "    X_base_scaled = scaler.fit_transform(X_base)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    X_without_product = X_base_scaled\n",
    "    X_with_product = np.column_stack([X_base_scaled, data['Product_Type_encoded'].values.reshape(-1, 1)])\n",
    "    y_storage = data['Storage_Condition_encoded'].values\n",
    "    \n",
    "    print(f\"\\nDataset shapes:\")\n",
    "    print(f\"X without product type: {X_without_product.shape}\")\n",
    "    print(f\"X with product type: {X_with_product.shape}\")\n",
    "    print(f\"Storage conditions: {dict(zip(le_storage.classes_, range(len(le_storage.classes_))))}\")\n",
    "    \n",
    "    return X_with_product, X_without_product, y_storage, le_storage.classes_, feature_cols\n",
    "\n",
    "def load_best_parameters():\n",
    "    \"\"\"Load best parameters from JSON file\"\"\"\n",
    "    try:\n",
    "        with open('hyperparameter_tuning/storage_condition_results.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Could not find hyperparameter tuning results file.\")\n",
    "        print(\"Please run hyperparameter tuning first.\")\n",
    "        return None\n",
    "\n",
    "def create_models(best_params, scenario='with_product_type'):\n",
    "    \"\"\"Create models with best parameters\"\"\"\n",
    "    models = {}\n",
    "    params = best_params[scenario]\n",
    "    \n",
    "    for model_name, model_params in params.items():\n",
    "        if model_name == 'SVM':\n",
    "            models[model_name] = SVC(**model_params, random_state=42, probability=True)\n",
    "        elif model_name == 'Random Forest':\n",
    "            models[model_name] = RandomForestClassifier(**model_params, random_state=42)\n",
    "        elif model_name == 'KNN':\n",
    "            models[model_name] = KNeighborsClassifier(**model_params)\n",
    "        elif model_name == 'Neural Network':\n",
    "            models[model_name] = MLPClassifier(**model_params, random_state=42)\n",
    "        elif model_name == 'Logistic Regression':\n",
    "            models[model_name] = LogisticRegression(**model_params, random_state=42)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Functions\n",
    "\n",
    "These functions implement a rigorous cross-validation framework for model evaluation:\n",
    "1. `evaluate_models`: Performs stratified k-fold cross-validation\n",
    "2. `evaluate_roc_auc`: Calculates and visualizes ROC curves and AUC scores\n",
    "\n",
    "The evaluation maintains consistency with the 5-fold cross-validation used during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Advanced Model Evaluation Functions\n",
    "def evaluate_models(X, y, models, cv=CV_SPLITS):\n",
    "    \"\"\"Evaluate models using cross-validation\"\"\"\n",
    "    results = {}\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        fold_accuracies = []\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        \n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "            # Split data (already scaled)\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # Fit and predict\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            fold_accuracies.append(acc)\n",
    "            all_predictions.extend(y_pred)\n",
    "            all_true_labels.extend(y_test)\n",
    "            \n",
    "            print(f\"Fold {fold_idx + 1} accuracy: {acc:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'fold_accuracies': np.array(fold_accuracies),\n",
    "            'mean_accuracy': np.mean(fold_accuracies),\n",
    "            'std_accuracy': np.std(fold_accuracies),\n",
    "            'confusion_matrix': confusion_matrix(all_true_labels, all_predictions),\n",
    "            'classification_report': classification_report(all_true_labels, all_predictions)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{model_name} Final Results:\")\n",
    "        print(f\"Mean accuracy: {results[model_name]['mean_accuracy']:.4f} Â± {results[model_name]['std_accuracy']:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(results[model_name]['classification_report'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_roc_auc(X, y, models, class_labels):\n",
    "    \"\"\"Calculate and visualize ROC curves and AUC scores for multi-class models\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nCalculating ROC AUC for {model_name}...\")\n",
    "        \n",
    "        # Skip models that can't produce probability predictions\n",
    "        if not hasattr(model, \"predict_proba\"):\n",
    "            print(f\"  Warning: {model_name} doesn't support probability predictions, skipping ROC AUC\")\n",
    "            continue\n",
    "            \n",
    "        # Get cross-validated probabilities\n",
    "        y_proba = cross_val_predict(\n",
    "            model, X, y, \n",
    "            cv=CV_SPLITS,\n",
    "            method='predict_proba',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Calculate ROC AUC for each class (one-vs-rest)\n",
    "        roc_auc_scores = {}\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for i, class_name in enumerate(class_labels):\n",
    "            # Convert to binary problem (current class vs the rest)\n",
    "            y_binary = (y == i).astype(int)\n",
    "            y_score = y_proba[:, i]\n",
    "            \n",
    "            # Calculate ROC curve and AUC\n",
    "            fpr, tpr, _ = roc_curve(y_binary, y_score)\n",
    "            auc_score = auc(fpr, tpr)\n",
    "            roc_auc_scores[class_name] = auc_score\n",
    "            \n",
    "            # Plot ROC curve for this class\n",
    "            plt.plot(fpr, tpr, label=f'{class_name} (AUC = {auc_score:.3f})')\n",
    "        \n",
    "        # Calculate macro average\n",
    "        macro_auc = np.mean(list(roc_auc_scores.values()))\n",
    "        roc_auc_scores['macro_avg'] = macro_auc\n",
    "        \n",
    "        # Finalize plot\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves for {model_name} (Macro AUC = {macro_auc:.3f})')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        # Save the plot\n",
    "        filename = f'roc_curve_{model_name.lower().replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, filename))\n",
    "        plt.close()\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = roc_auc_scores\n",
    "        \n",
    "        # Print results summary\n",
    "        print(f\"  ROC AUC Results:\")\n",
    "        for class_name, score in roc_auc_scores.items():\n",
    "            print(f\"  - {class_name}: {score:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_feature_importance(model, X, feature_names, top_n=20, include_product_type=False):\n",
    "    \"\"\"Analyze and visualize feature importance from tree-based models\"\"\"\n",
    "    # Check if model supports feature importance\n",
    "    if not hasattr(model, 'feature_importances_'):\n",
    "        print(f\"Model {type(model).__name__} doesn't support feature_importances_ attribute.\")\n",
    "        return None\n",
    "    \n",
    "    # Get feature importance\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Get indices of top features\n",
    "    indices = np.argsort(importances)[::-1][:top_n]\n",
    "    \n",
    "    # Create feature labels\n",
    "    if include_product_type and len(feature_names) < X.shape[1]:\n",
    "        # Last feature is product type if included\n",
    "        all_features = list(feature_names) + ['Product Type']\n",
    "    else:\n",
    "        all_features = feature_names\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(f\"Top {top_n} Features by Importance\")\n",
    "    plt.bar(range(len(indices)), importances[indices], align=\"center\")\n",
    "    plt.xticks(range(len(indices)), [all_features[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    model_name = type(model).__name__.lower()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'feature_importance_{model_name}.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Return top features and their importance\n",
    "    return [(all_features[i], importances[i]) for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: Visualization Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Comparison Functions\n",
    "\n",
    "These functions create visualizations to compare model performance:\n",
    "1. `plot_model_comparison`: Bar charts showing accuracy with/without product type\n",
    "2. `plot_confusion_matrices`: Confusion matrices for each model configuration\n",
    "3. `compare_roc_auc`: ROC curve analysis for multi-class classification\n",
    "4. `plot_learning_curves`: Learning curves to assess model training dynamics\n",
    "5. `analyze_feature_importance`: Feature importance visualization for tree-based models\n",
    "\n",
    "These visualizations help identify patterns and insights in model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Visualization Functions\n",
    "def plot_model_comparison(results_without, results_with):\n",
    "    \"\"\"Plot model comparison bar chart\"\"\"\n",
    "    models = list(results_without.keys())\n",
    "    \n",
    "    means_without = [results_without[model]['mean_accuracy'] for model in models]\n",
    "    stds_without = [results_without[model]['std_accuracy'] for model in models]\n",
    "    \n",
    "    means_with = [results_with[model]['mean_accuracy'] for model in models]\n",
    "    stds_with = [results_with[model]['std_accuracy'] for model in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    rects1 = ax.bar(x - width/2, means_without, width, yerr=stds_without,\n",
    "                    label='Without Product Type', capsize=5)\n",
    "    rects2 = ax.bar(x + width/2, means_with, width, yerr=stds_with,\n",
    "                    label='With Product Type', capsize=5)\n",
    "    \n",
    "    ax.set_ylabel('Mean Accuracy')\n",
    "    ax.set_title('Model Comparison for Storage Condition Classification')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate(f'{height:.3f}',\n",
    "                       xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom')\n",
    "    \n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'storage_condition_comparison.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrices(results_without, results_with, class_labels):\n",
    "    \"\"\"Plot confusion matrices side by side for with/without product type\"\"\"\n",
    "    for model_name in results_without.keys():\n",
    "        # Create two subplots side by side\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot first confusion matrix (Without Product Type)\n",
    "        sns.heatmap(results_without[model_name]['confusion_matrix'], \n",
    "                   annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=class_labels,  # Use actual class labels\n",
    "                   yticklabels=class_labels, ax=ax1)\n",
    "        ax1.set_title(f'{model_name} - Without Product Type')\n",
    "        ax1.set_ylabel('Actual')\n",
    "        ax1.set_xlabel('Predicted')\n",
    "        \n",
    "        # Plot second confusion matrix (With Product Type)\n",
    "        sns.heatmap(results_with[model_name]['confusion_matrix'], \n",
    "                   annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=class_labels,  # Use actual class labels\n",
    "                   yticklabels=class_labels, ax=ax2)\n",
    "        ax2.set_title(f'{model_name} - With Product Type')\n",
    "        ax2.set_ylabel('Actual')\n",
    "        ax2.set_xlabel('Predicted')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, filename))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_roc_auc(roc_results_with, roc_results_without, class_labels):\n",
    "    \"\"\"Compare ROC AUC performance with and without product type\"\"\"\n",
    "    print(\"\\nROC AUC Performance Comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create data for plotting\n",
    "    models = []\n",
    "    auc_with = []\n",
    "    auc_without = []\n",
    "    \n",
    "    for model_name in roc_results_with.keys():\n",
    "        if model_name in roc_results_without:\n",
    "            models.append(model_name)\n",
    "            auc_with.append(roc_results_with[model_name]['macro_avg'])\n",
    "            auc_without.append(roc_results_without[model_name]['macro_avg'])\n",
    "            \n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"  - Macro AUC with product type: {roc_results_with[model_name]['macro_avg']:.4f}\")\n",
    "            print(f\"  - Macro AUC without product type: {roc_results_without[model_name]['macro_avg']:.4f}\")\n",
    "            print(f\"  - Difference: {roc_results_with[model_name]['macro_avg'] - roc_results_without[model_name]['macro_avg']:.4f}\")\n",
    "            \n",
    "            # Check for class-specific differences\n",
    "            print(\"  - Class-specific differences:\")\n",
    "            for class_name in class_labels:\n",
    "                diff = roc_results_with[model_name][class_name] - roc_results_without[model_name][class_name]\n",
    "                print(f\"    â¢ {class_name}: {diff:.4f}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(x - width/2, auc_with, width, label='With Product Type')\n",
    "    plt.bar(x + width/2, auc_without, width, label='Without Product Type')\n",
    "    \n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Macro ROC AUC')\n",
    "    plt.title('ROC AUC Comparison (With vs. Without Product Type)')\n",
    "    plt.xticks(x, models, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'roc_auc_comparison.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_learning_curves(X, y, model, model_name, cv=CV_SPLITS):\n",
    "    \"\"\"Plot learning curves with confidence intervals\"\"\"\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y,\n",
    "        train_sizes=train_sizes,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, label='Training Score', color='blue', marker='o')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n",
    "                     alpha=0.15, color='blue')\n",
    "    plt.plot(train_sizes, val_mean, label='Cross-validation Score', color='red', marker='o')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, \n",
    "                     alpha=0.15, color='red')\n",
    "    \n",
    "    plt.title(f'Learning Curves - {model_name}')\n",
    "    plt.xlabel('Training Examples')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    final_train = f\"Final training score: {train_mean[-1]:.4f} Â± {train_std[-1]:.4f}\"\n",
    "    final_val = f\"Final validation score: {val_mean[-1]:.4f} Â± {val_std[-1]:.4f}\"\n",
    "    plt.annotate(final_train, xy=(0.6, 0.2), xycoords='axes fraction')\n",
    "    plt.annotate(final_val, xy=(0.6, 0.15), xycoords='axes fraction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'learning_curve_{model_name.lower().replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, filename))\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'train_sizes': train_sizes,\n",
    "        'train_scores': {'mean': train_mean, 'std': train_std},\n",
    "        'val_scores': {'mean': val_mean, 'std': val_std}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis Functions\n",
    "\n",
    "These functions perform statistical analyses to determine if differences in model performance are significant:\n",
    "1. `perform_statistical_analysis`: Conducts paired t-tests to compare models with/without product type\n",
    "2. `save_analysis_summary`: Generates a comprehensive report of findings\n",
    "\n",
    "The statistical analysis helps determine whether including product type as a feature significantly improves classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Statistical Analysis Functions\n",
    "def perform_statistical_analysis(results_without, results_with):\n",
    "    \"\"\"Perform statistical analysis\"\"\"\n",
    "    analysis_results = []\n",
    "    \n",
    "    print(\"\\nStatistical Analysis Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for model in results_without.keys():\n",
    "        acc_without = results_without[model]['fold_accuracies']\n",
    "        acc_with = results_with[model]['fold_accuracies']\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_rel(acc_with, acc_without)\n",
    "        improvement = (np.mean(acc_with) - np.mean(acc_without)) * 100\n",
    "        \n",
    "        result = {\n",
    "            'Model': model,\n",
    "            'Accuracy without Product': f\"{np.mean(acc_without):.4f} Â± {np.std(acc_without):.4f}\",\n",
    "            'Accuracy with Product': f\"{np.mean(acc_with):.4f} Â± {np.std(acc_with):.4f}\",\n",
    "            'Improvement (%)': f\"{improvement:.2f}%\",\n",
    "            'p-value': f\"{p_value:.4f}\",\n",
    "            'Significant': \"Yes\" if p_value < 0.05 else \"No\"\n",
    "        }\n",
    "        analysis_results.append(result)\n",
    "        \n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"- Without Product Type: {result['Accuracy without Product']}\")\n",
    "        print(f\"- With Product Type: {result['Accuracy with Product']}\")\n",
    "        print(f\"- Improvement: {improvement:.2f}%\")\n",
    "        print(f\"- Statistical Significance (p < 0.05): {result['Significant']} (p = {p_value:.4f})\")\n",
    "\n",
    "    results_df = pd.DataFrame(analysis_results)\n",
    "    results_df.to_csv(os.path.join(RESULTS_DIR, 'statistical_analysis_results.csv'), index=False)\n",
    "    return results_df\n",
    "\n",
    "def save_analysis_summary(results_without, results_with, stats_results, roc_results_with=None, roc_results_without=None):\n",
    "    \"\"\"Generate and save comprehensive analysis summary\"\"\"\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, 'analysis_summary.txt'), 'w') as f:\n",
    "        # Header\n",
    "        f.write(\"Storage Condition Classification Analysis Summary\\n\")\n",
    "        f.write(\"==================================================\\n\\n\")\n",
    "        \n",
    "        # 1. Best Performing Models section\n",
    "        f.write(\"1. Best Performing Models:\\n\\n\")\n",
    "        \n",
    "        # Find best models\n",
    "        best_without = max(results_without.items(), \n",
    "                         key=lambda x: x[1]['mean_accuracy'])\n",
    "        best_with = max(results_with.items(), \n",
    "                       key=lambda x: x[1]['mean_accuracy'])\n",
    "        \n",
    "        # Without Product Type\n",
    "        f.write(\"Without Product Type:\\n\")\n",
    "        f.write(f\"- Best Model: {best_without[0]}\\n\")\n",
    "        f.write(f\"- Accuracy: {best_without[1]['mean_accuracy']:.4f} Â± \"\n",
    "                f\"{best_without[1]['std_accuracy']:.4f}\\n\\n\")\n",
    "        \n",
    "        # With Product Type\n",
    "        f.write(\"With Product Type:\\n\")\n",
    "        f.write(f\"- Best Model: {best_with[0]}\\n\")\n",
    "        f.write(f\"- Accuracy: {best_with[1]['mean_accuracy']:.4f} Â± \"\n",
    "                f\"{best_with[1]['std_accuracy']:.4f}\\n\\n\")\n",
    "        \n",
    "        # 2. Impact of Product Type section\n",
    "        f.write(\"2. Impact of Product Type:\\n\")\n",
    "        \n",
    "        # Create DataFrame for formatted table\n",
    "        data = []\n",
    "        for model_name in results_without.keys():\n",
    "            acc_without = results_without[model_name]['mean_accuracy']\n",
    "            std_without = results_without[model_name]['std_accuracy']\n",
    "            \n",
    "            acc_with = results_with[model_name]['mean_accuracy']\n",
    "            std_with = results_with[model_name]['std_accuracy']\n",
    "            \n",
    "            improvement = ((acc_with - acc_without) / acc_without) * 100\n",
    "            \n",
    "            # Get p-value from stats_results DataFrame\n",
    "            p_value = float(stats_results[stats_results['Model'] == model_name]['p-value'].values[0])\n",
    "            \n",
    "            data.append({\n",
    "                'Model': model_name,\n",
    "                'Accuracy without Product': f\"{acc_without:.4f} Â± {std_without:.4f}\",\n",
    "                'Accuracy with Product': f\"{acc_with:.4f} Â± {std_with:.4f}\",\n",
    "                'Improvement (%)': f\"{improvement:.2f}%\",\n",
    "                'p-value': f\"{p_value:.4f}\",\n",
    "                'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame and write as formatted table\n",
    "        df = pd.DataFrame(data)\n",
    "        f.write(df.to_string(index=False))\n",
    "        \n",
    "        # 3. ROC AUC Analysis if available\n",
    "        if roc_results_with and roc_results_without:\n",
    "            f.write(\"\\n\\n3. ROC AUC Analysis:\\n\")\n",
    "            f.write(\"-----------------\\n\")\n",
    "            \n",
    "            for model_name in roc_results_with.keys():\n",
    "                if model_name in roc_results_without:\n",
    "                    auc_with = roc_results_with[model_name]['macro_avg']\n",
    "                    auc_without = roc_results_without[model_name]['macro_avg']\n",
    "                    \n",
    "                    f.write(f\"\\n{model_name}:\\n\")\n",
    "                    f.write(f\"- Macro AUC with product type: {auc_with:.4f}\\n\")\n",
    "                    f.write(f\"- Macro AUC without product type: {auc_without:.4f}\\n\")\n",
    "                    f.write(f\"- Difference: {auc_with - auc_without:.4f}\\n\")\n",
    "        \n",
    "        # 4. Additional Insights\n",
    "        section_num = 4 if (roc_results_with and roc_results_without) else 3\n",
    "        f.write(f\"\\n\\n{section_num}. Additional Insights:\\n\")\n",
    "        f.write(\"-----------------------\\n\")\n",
    "        \n",
    "        # Count models with significant improvement\n",
    "        significant_improvements = sum(1 for d in data if float(d['Improvement (%)'].strip('%')) > 0 \n",
    "                                    and d['Significant'] == 'Yes')\n",
    "        f.write(f\"\\nNumber of models with significant improvement: {significant_improvements}\\n\")\n",
    "        \n",
    "        # Best improvement\n",
    "        best_improvement = max(data, key=lambda x: float(x['Improvement (%)'].strip('%')))\n",
    "        f.write(f\"Model with best improvement: {best_improvement['Model']} \"\n",
    "                f\"({best_improvement['Improvement (%)']}) \"\n",
    "                f\"(Significant: {best_improvement['Significant']})\\n\")\n",
    "        \n",
    "        # Overall recommendation\n",
    "        f.write(\"\\nOverall Recommendation:\\n\")\n",
    "        if significant_improvements > 0:\n",
    "            f.write(\"Including product type information appears beneficial for classification performance.\\n\")\n",
    "        else:\n",
    "            f.write(\"Product type information does not significantly improve classification performance.\\n\")\n",
    "            best_model_name = best_without[0]\n",
    "            f.write(f\"Consider using {best_model_name} without product type for simplicity and generalization.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "This section brings together all components in a structured workflow:\n",
    "1. Prepare data with consistent preprocessing\n",
    "2. Load optimized models from hyperparameter tuning results\n",
    "3. Evaluate models using cross-validation\n",
    "4. Generate visualizations for performance comparison\n",
    "5. Analyze feature importance for the Random Forest model\n",
    "6. Perform statistical tests to assess significance\n",
    "7. Save comprehensive analysis reports and visualizations\n",
    "\n",
    "Running this cell executes the complete analysis pipeline and saves results to the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Prepare Data\n",
    "    X_with_product, X_without_product, y_storage, class_labels, feature_cols = prepare_data()\n",
    "    \n",
    "    # 2. Load Models\n",
    "    print(\"Loading best parameters and creating models...\")\n",
    "    best_params = load_best_parameters()\n",
    "    if best_params is None:\n",
    "        print(\"Error: Could not load hyperparameter results. Exiting.\")\n",
    "        import sys\n",
    "        sys.exit(1)\n",
    "        \n",
    "    models_with_product = create_models(best_params, 'with_product_type')\n",
    "    models_without_product = create_models(best_params, 'without_product_type')\n",
    "    \n",
    "    # 3. Evaluate Models\n",
    "    print(\"\\nEvaluating models with product type...\")\n",
    "    results_with_product = evaluate_models(X_with_product, y_storage, models_with_product)\n",
    "    \n",
    "    print(\"\\nEvaluating models without product type...\")\n",
    "    results_without_product = evaluate_models(X_without_product, y_storage, models_without_product)\n",
    "    \n",
    "    # 4. Generate Visualizations and Analysis\n",
    "    print(\"\\nGenerating visualizations and analysis...\")\n",
    "    \n",
    "    # Model comparison plot\n",
    "    plot_model_comparison(results_without_product, results_with_product)\n",
    "    \n",
    "    # Confusion matrices with actual class labels\n",
    "    plot_confusion_matrices(results_without_product, results_with_product, class_labels)\n",
    "    \n",
    "    # ROC AUC Analysis\n",
    "    print(\"\\nPerforming ROC AUC analysis...\")\n",
    "    roc_results_with = evaluate_roc_auc(X_with_product, y_storage, models_with_product, class_labels)\n",
    "    roc_results_without = evaluate_roc_auc(X_without_product, y_storage, models_without_product, class_labels)\n",
    "    compare_roc_auc(roc_results_with, roc_results_without, class_labels)\n",
    "    \n",
    "    # Feature Importance Analysis\n",
    "    print(\"\\nAnalyzing feature importance...\")\n",
    "    if 'Random Forest' in models_with_product:\n",
    "        rf_model = models_with_product['Random Forest']\n",
    "        top_features = analyze_feature_importance(\n",
    "            rf_model, X_with_product, feature_cols, \n",
    "            top_n=20, include_product_type=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\nTop 10 most important features:\")\n",
    "        for i, (feature, importance) in enumerate(top_features[:10]):\n",
    "            print(f\"{i+1}. {feature}: {importance:.4f}\")\n",
    "    \n",
    "    # Statistical analysis\n",
    "    stats_results = perform_statistical_analysis(results_without_product, results_with_product)\n",
    "    \n",
    "    # Save analysis summary\n",
    "    save_analysis_summary(results_without_product, results_with_product, \n",
    "                         stats_results, roc_results_with, roc_results_without)\n",
    "\n",
    "    # Learning curves\n",
    "    print(\"\\nGenerating learning curves...\")\n",
    "    for model_name, model in models_with_product.items():\n",
    "        print(f\"Generating learning curve for {model_name}...\")\n",
    "        plot_learning_curves(X_with_product, y_storage, model, model_name)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    print(f\"All results saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
