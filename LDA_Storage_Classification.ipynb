{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "from itertools import cycle\n",
    "\n",
    "# Import models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('augmented_bakery_data.csv')\n",
    "sns.scatterplot(x='gain_0',y='gain_100',hue='Product_Type', data=all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and Prepare Data\n",
    "print(\"Loading and preparing data...\")\n",
    "# Load data\n",
    "data = pd.read_csv('augmented_bakery_data.csv')\n",
    "\n",
    "# Encode Product Type\n",
    "le_product = LabelEncoder()\n",
    "data['Product_Type_encoded'] = le_product.fit_transform(data['Product_Type'])\n",
    "\n",
    "# One-hot encode Storage Conditions\n",
    "storage_dummies = pd.get_dummies(data['Storage_Condition'], prefix='storage')\n",
    "data = pd.concat([data, storage_dummies], axis=1)\n",
    "\n",
    "print(\"\\nProduct Type Encoding:\")\n",
    "for i, label in enumerate(le_product.classes_):\n",
    "    print(f\"{label}: {i}\")\n",
    "\n",
    "print(\"\\nStorage Condition Columns:\")\n",
    "print(storage_dummies.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Prepare Features and Target\n",
    "print(\"\\nPreparing features...\")\n",
    "# Get base features (gains and phases)\n",
    "feature_cols = [col for col in data.columns if col.startswith(('gain_', 'phase_'))]\n",
    "X_without_storage = data[feature_cols].copy()\n",
    "y_product = data['Product_Type_encoded']\n",
    "\n",
    "# Prepare features with storage conditions\n",
    "storage_cols = [col for col in data.columns if col.startswith('storage_')]\n",
    "X_with_storage = pd.concat([X_without_storage, data[storage_cols]], axis=1)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_without_storage_scaled = scaler.fit_transform(X_without_storage)\n",
    "X_with_storage_scaled = scaler.fit_transform(X_with_storage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "feature_cols = [col for col in all_data.columns if col.startswith(('gain_', 'phase_'))]\n",
    "X = all_data[feature_cols]\n",
    "y = all_data['Storage_Condition']\n",
    "\n",
    "# LDA for Storage Condition with n_components=2\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "\n",
    "# Plot LDA results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_lda[:, 0], y=X_lda[:, 1], hue=y, palette='Set2')\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.ylabel('LDA Component 2')\n",
    "plt.title('LDA of PCA Components for Storage Condition')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "RANDOM_STATE = 42\n",
    "CV_SPLITS = 10\n",
    "\n",
    "# Cell 2: Load Best Parameters and Create Models\n",
    "def load_best_parameters():\n",
    "    \"\"\"Load best parameters from JSON file\"\"\"\n",
    "    with open('hyperparameter_tuning/LDA_storage_condition_results.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "RESULTS_DIR = 'results/LDA4Storage_Condition_Classification'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "        \n",
    "def create_models(best_params, scenario='with_product_type'):\n",
    "    \"\"\"Create models with best parameters\"\"\"\n",
    "    models = {}\n",
    "    params = best_params[scenario]\n",
    "    \n",
    "    for model_name, model_params in params.items():\n",
    "        if model_name == 'SVM':\n",
    "            models[model_name] = SVC(**model_params, random_state=42, probability=True)\n",
    "        elif model_name == 'Random Forest':\n",
    "            models[model_name] = RandomForestClassifier(**model_params, random_state=42)\n",
    "        elif model_name == 'KNN':\n",
    "            models[model_name] = KNeighborsClassifier(**model_params)\n",
    "        elif model_name == 'Neural Network':\n",
    "            models[model_name] = MLPClassifier(**model_params, random_state=42)\n",
    "        elif model_name == 'Logistic Regression':\n",
    "            models[model_name] = LogisticRegression(**model_params, random_state=42)\n",
    "    \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 3: Visualization Functions\n",
    "def plot_model_comparison(results_without, results_with):\n",
    "    \"\"\"Plot model comparison bar chart\"\"\"\n",
    "    models = list(results_without.keys())\n",
    "    \n",
    "    means_without = [results_without[model]['mean_accuracy'] for model in models]\n",
    "    stds_without = [results_without[model]['std_accuracy'] for model in models]\n",
    "    \n",
    "    means_with = [results_with[model]['mean_accuracy'] for model in models]\n",
    "    stds_with = [results_with[model]['std_accuracy'] for model in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    rects1 = ax.bar(x - width/2, means_without, width, yerr=stds_without,\n",
    "                    label='Without Product Type', capsize=5)\n",
    "    rects2 = ax.bar(x + width/2, means_with, width, yerr=stds_with,\n",
    "                    label='With Product Type', capsize=5)\n",
    "    \n",
    "    ax.set_ylabel('Mean Accuracy')\n",
    "    ax.set_title('Model Comparison for Storage Condition Classification')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate(f'{height:.3f}',\n",
    "                       xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom')\n",
    "    \n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'storage_condition_comparison.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrices(results_without, results_with, class_labels):\n",
    "    \"\"\"Plot confusion matrices side by side for with/without product type\"\"\"\n",
    "    for model_name in results_without.keys():\n",
    "        # Create two subplots side by side\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot first confusion matrix (Without Product Type)\n",
    "        sns.heatmap(results_without[model_name]['confusion_matrix'], \n",
    "                   annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=range(len(class_labels)),\n",
    "                   yticklabels=range(len(class_labels)), ax=ax1)\n",
    "        ax1.set_title(f'{model_name} - Without Product Type')\n",
    "        ax1.set_ylabel('Actual')\n",
    "        ax1.set_xlabel('Predicted')\n",
    "        \n",
    "        # Plot second confusion matrix (With Product Type)\n",
    "        sns.heatmap(results_with[model_name]['confusion_matrix'], \n",
    "                   annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=range(len(class_labels)),\n",
    "                   yticklabels=range(len(class_labels)), ax=ax2)\n",
    "        ax2.set_title(f'{model_name} - With Product Type')\n",
    "        ax2.set_ylabel('Actual')\n",
    "        ax2.set_xlabel('Predicted')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, filename))\n",
    "        plt.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def plot_learning_curves(X, y, model, model_name, cv=CV_SPLITS):\n",
    "    \"\"\"Plot learning curves with confidence intervals\"\"\"\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    #cv = StratifiedKFold(n_splits=CV_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y,\n",
    "        train_sizes=train_sizes,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, label='Training Score', color='blue', marker='o')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n",
    "                     alpha=0.15, color='blue')\n",
    "    plt.plot(train_sizes, val_mean, label='Cross-validation Score', color='red', marker='o')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, \n",
    "                     alpha=0.15, color='red')\n",
    "    \n",
    "    plt.title(f'Learning Curves - {model_name}')\n",
    "    plt.xlabel('Training Examples')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    final_train = f\"Final training score: {train_mean[-1]:.4f} ± {train_std[-1]:.4f}\"\n",
    "    final_val = f\"Final validation score: {val_mean[-1]:.4f} ± {val_std[-1]:.4f}\"\n",
    "    plt.annotate(final_train, xy=(0.6, 0.2), xycoords='axes fraction')\n",
    "    plt.annotate(final_val, xy=(0.6, 0.15), xycoords='axes fraction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f'learning_curve_{model_name.lower().replace(\" \", \"_\")}.png')\n",
    "    #plt.show()\n",
    "    # Save plot\n",
    "    filename = f'learning_curve_{model_name.lower().replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, filename))\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'train_sizes': train_sizes,\n",
    "        'train_scores': {'mean': train_mean, 'std': train_std},\n",
    "        'val_scores': {'mean': val_mean, 'std': val_std}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_statistical_analysis(results_without, results_with):\n",
    "    \"\"\"Perform statistical analysis\"\"\"\n",
    "    analysis_results = []\n",
    "    \n",
    "    print(\"\\nStatistical Analysis Results:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model in results_without.keys():\n",
    "        acc_without = results_without[model]['fold_accuracies']\n",
    "        acc_with = results_with[model]['fold_accuracies']\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_rel(acc_with, acc_without)\n",
    "        improvement = (np.mean(acc_with) - np.mean(acc_without)) * 100\n",
    "        \n",
    "        result = {\n",
    "            'Model': model,\n",
    "            'Accuracy without Product': f\"{np.mean(acc_without):.4f} ± {np.std(acc_without):.4f}\",\n",
    "            'Accuracy with Product': f\"{np.mean(acc_with):.4f} ± {np.std(acc_with):.4f}\",\n",
    "            'Improvement (%)': f\"{improvement:.2f}%\",\n",
    "            'p-value': f\"{p_value:.4f}\",\n",
    "            'Significant': \"Yes\" if p_value < 0.05 else \"No\"\n",
    "        }\n",
    "        analysis_results.append(result)\n",
    "        \n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"- Without Product Type: {result['Accuracy without Product']}\")\n",
    "        print(f\"- With Product Type: {result['Accuracy with Product']}\")\n",
    "        print(f\"- Improvement: {result['Improvement (%)']}\")\n",
    "        print(f\"- Statistical Significance (p < 0.05): {result['Significant']} (p = {result['p-value']})\")\n",
    "\n",
    "    results_df = pd.DataFrame(analysis_results)\n",
    "    results_df.to_csv('statistical_analysis_results.csv', index=False)\n",
    "    return results_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Data Preparation\n",
    "def prepare_data():\n",
    "    \"\"\"Load and prepare data consistently\"\"\"\n",
    "    print(\"Loading data and preparing features...\")\n",
    "    data = pd.read_csv('processed_bakery_data.csv')\n",
    "    \n",
    "    # Encode labels\n",
    "    le_product = LabelEncoder()\n",
    "    le_storage = LabelEncoder()\n",
    "    data['Product_Type_encoded'] = le_product.fit_transform(data['Product_Type'])\n",
    "    data['Storage_Condition_encoded'] = le_storage.fit_transform(data['Storage_Condition'])\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_cols = [col for col in data.columns if col.startswith(('gain_', 'phase_'))]\n",
    "    X_base = data[feature_cols]\n",
    "    \n",
    "    # Scale features on entire dataset\n",
    "    scaler = StandardScaler()\n",
    "    X_base_scaled = scaler.fit_transform(X_base)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    X_without_product = X_base_scaled\n",
    "    X_with_product = np.column_stack([X_base_scaled, data['Product_Type_encoded'].values.reshape(-1, 1)])\n",
    "    y_storage = data['Storage_Condition_encoded'].values\n",
    "    \n",
    "    return X_with_product, X_without_product, y_storage, le_storage.classes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import cross_val_predict\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def evaluate_models(X, y, models, cv=CV_SPLITS):\n",
    "    \"\"\"Evaluate models using cross-validation with scikit-learn built-in functions and LDA\"\"\"\n",
    "    results = {}\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    num_classes = len(np.unique(y))  # Number of unique classes in the target variable\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        # Create a pipeline with LDA and the model\n",
    "        pipeline = Pipeline([\n",
    "            ('lda', LDA(n_components=num_classes - 1)),  # LDA for dimensionality reduction\n",
    "            (model_name, model)  # The model to evaluate\n",
    "        ])\n",
    "        \n",
    "        # Use cross_val_score to compute fold-wise accuracies\n",
    "        cv_scores = cross_val_score(pipeline, X, y, cv=skf, scoring='accuracy', n_jobs=-1)\n",
    "        \n",
    "        # Use cross_val_predict to get predictions for the entire dataset\n",
    "        y_pred = cross_val_predict(pipeline, X, y, cv=skf, n_jobs=-1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mean_accuracy = cv_scores.mean()\n",
    "        std_accuracy = cv_scores.std()\n",
    "        conf_matrix = confusion_matrix(y, y_pred)\n",
    "        clf_report = classification_report(y, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'fold_accuracies': cv_scores,\n",
    "            'mean_accuracy': mean_accuracy,\n",
    "            'std_accuracy': std_accuracy,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'classification_report': clf_report\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{model_name} Final Results:\")\n",
    "        print(f\"Mean accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(clf_report)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_summary(results_without, results_with, stats_results):\n",
    "    \"\"\"Generate and save comprehensive analysis summary\"\"\"\n",
    "    \n",
    "    with open('analysis_summary.txt', 'w') as f:\n",
    "        # Header\n",
    "        f.write(\"Storage Condition Classification Analysis Summary\\n\")\n",
    "        f.write(\"==================================================\\n\\n\")\n",
    "        \n",
    "        # 1. Best Performing Models section\n",
    "        f.write(\"1. Best Performing Models:\\n\\n\")\n",
    "        \n",
    "        # Find best models\n",
    "        best_without = max(results_without.items(), \n",
    "                         key=lambda x: x[1]['mean_accuracy'])\n",
    "        best_with = max(results_with.items(), \n",
    "                       key=lambda x: x[1]['mean_accuracy'])\n",
    "        \n",
    "        # Without Product Type\n",
    "        f.write(\"Without Product Type:\\n\")\n",
    "        f.write(f\"- Best Model: {best_without[0]}\\n\")\n",
    "        f.write(f\"- Accuracy: {best_without[1]['mean_accuracy']:.4f} ± \"\n",
    "                f\"{best_without[1]['std_accuracy']:.4f}\\n\\n\")\n",
    "        \n",
    "        # With Product Type\n",
    "        f.write(\"With Product Type:\\n\")\n",
    "        f.write(f\"- Best Model: {best_with[0]}\\n\")\n",
    "        f.write(f\"- Accuracy: {best_with[1]['mean_accuracy']:.4f} ± \"\n",
    "                f\"{best_with[1]['std_accuracy']:.4f}\\n\\n\")\n",
    "        \n",
    "        # 2. Impact of Product Type section\n",
    "        f.write(\"2. Impact of Product Type:\\n\")\n",
    "        \n",
    "        # Create DataFrame for formatted table\n",
    "        data = []\n",
    "        for model_name in results_without.keys():\n",
    "            acc_without = results_without[model_name]['mean_accuracy']\n",
    "            std_without = results_without[model_name]['std_accuracy']\n",
    "            \n",
    "            acc_with = results_with[model_name]['mean_accuracy']\n",
    "            std_with = results_with[model_name]['std_accuracy']\n",
    "            \n",
    "            improvement = ((acc_with - acc_without) / acc_without) * 100\n",
    "            \n",
    "            # Get p-value from stats_results DataFrame\n",
    "            p_value = float(stats_results[stats_results['Model'] == model_name]['p-value'].values[0].strip())\n",
    "            \n",
    "            data.append({\n",
    "                'Model': model_name,\n",
    "                'Accuracy without Product': f\"{acc_without:.4f} ± {std_without:.4f}\",\n",
    "                'Accuracy with Product': f\"{acc_with:.4f} ± {std_with:.4f}\",\n",
    "                'Improvement (%)': f\"{improvement:.2f}%\",\n",
    "                'p-value': f\"{p_value:.4f}\",\n",
    "                'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame and write as formatted table\n",
    "        df = pd.DataFrame(data)\n",
    "        f.write(df.to_string(index=False))\n",
    "        \n",
    "        # Additional Analysis\n",
    "        f.write(\"\\n\\n3. Additional Insights:\\n\")\n",
    "        f.write(\"-----------------------\\n\")\n",
    "        \n",
    "        # Count models with significant improvement\n",
    "        significant_improvements = sum(1 for d in data if float(d['Improvement (%)'].strip('%')) > 0 \n",
    "                                    and d['Significant'] == 'Yes')\n",
    "        f.write(f\"\\nNumber of models with significant improvement: {significant_improvements}\\n\")\n",
    "        \n",
    "        # Best improvement\n",
    "        best_improvement = max(data, key=lambda x: float(x['Improvement (%)'].strip('%')))\n",
    "        f.write(f\"Model with best improvement: {best_improvement['Model']} \"\n",
    "                f\"({best_improvement['Improvement (%)']})\\n\")\n",
    "        \n",
    "        # Overall recommendation\n",
    "        f.write(\"\\nOverall Recommendation:\\n\")\n",
    "        if significant_improvements > 0:\n",
    "            f.write(\"Including product type information appears beneficial for classification performance.\\n\")\n",
    "        else:\n",
    "            f.write(\"Product type information does not significantly improve classification performance.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Cell 6: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 6: Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Prepare Data\n",
    "    X_with_product, X_without_product, y_storage, class_labels = prepare_data()\n",
    "    \n",
    "    # 2. Load Models\n",
    "    print(\"Loading best parameters and creating models...\")\n",
    "    best_params = load_best_parameters()\n",
    "    models_with_product = create_models(best_params, 'with_product_type')\n",
    "    models_without_product = create_models(best_params, 'without_product_type')\n",
    "    \n",
    "    # 3. Evaluate Models\n",
    "    print(\"\\nEvaluating models with product type...\")\n",
    "    results_with_product = evaluate_models(X_with_product, y_storage, models_with_product)\n",
    "    \n",
    "    print(\"\\nEvaluating models without product type...\")\n",
    "    results_without_product = evaluate_models(X_without_product, y_storage, models_without_product)\n",
    "    \n",
    "    # 4. Generate Visualizations and Analysis\n",
    "    print(\"\\nGenerating visualizations and analysis...\")\n",
    "    \n",
    "    # Model comparison plot\n",
    "    plot_model_comparison(results_without_product, results_with_product)\n",
    "    \n",
    "    # Confusion matrices\n",
    "    plot_confusion_matrices(results_without_product, results_with_product, class_labels)\n",
    "    \n",
    "    # Statistical analysis\n",
    "    stats_results = perform_statistical_analysis(results_without_product, results_with_product)\n",
    "    \n",
    "    # Save analysis summary\n",
    "    save_analysis_summary(results_without_product, results_with_product, stats_results)\n",
    "\n",
    "    # Learning curves\n",
    "    for model_name, model in models_with_product.items():\n",
    "        print(f\"\\nGenerating learning curve for {model_name}...\")\n",
    "        plot_learning_curves(X_with_product, y_storage, model, model_name)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
