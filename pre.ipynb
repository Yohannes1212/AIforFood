{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def load_s1p_file(file_path):\n",
    "    \"\"\"\n",
    "    Load an s1p file and return frequency, gain, and phase data\n",
    "    Skip the metadata row (first row)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Skip the first row (metadata) and load the data\n",
    "        data = pd.read_csv(file_path, delimiter=r'\\s+', header=None, skiprows=1)\n",
    "        \n",
    "        # Verify we have 101 rows of data\n",
    "        if len(data) != 101:\n",
    "            print(f\"Warning: {file_path} has {len(data)} rows instead of 101\")\n",
    "            \n",
    "        frequency = data.iloc[:, 0]\n",
    "        gain = data.iloc[:, 1]\n",
    "        phase = data.iloc[:, 2]\n",
    "        \n",
    "        return frequency, gain, phase\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "def extract_metadata(filename):\n",
    "    \"\"\"\n",
    "    Extract product type and storage condition from filename\n",
    "    Example: A_1_1.s1p -> Product Type: A (bread), Storage: 1 (open)\n",
    "    \"\"\"\n",
    "    parts = os.path.basename(filename).split('_')\n",
    "    product_type = 'Bread' if parts[0] == 'A' else 'Cookies'\n",
    "    storage_condition = {\n",
    "        '1': 'Open',\n",
    "        '2': 'Wrapped',\n",
    "        '3': 'Humid'\n",
    "    }.get(parts[1], 'Unknown')\n",
    "    \n",
    "    return product_type, storage_condition\n",
    "\n",
    "def create_dataset(data_folder):\n",
    "    \"\"\"\n",
    "    Create a dataset from all s1p files in the folder\n",
    "    \"\"\"\n",
    "    # Get all s1p files\n",
    "    s1p_files = glob(os.path.join(data_folder, '*.s1p'))\n",
    "    s1p_files.sort()  # Sort files to ensure consistent ordering\n",
    "    \n",
    "    if not s1p_files:\n",
    "        raise ValueError(f\"No .s1p files found in {data_folder}\")\n",
    "\n",
    "    # Lists to store data\n",
    "    all_features = []\n",
    "    product_types = []\n",
    "    storage_conditions = []\n",
    "    filenames = []\n",
    "\n",
    "    # Process each file\n",
    "    for file_path in s1p_files:\n",
    "        # Load data\n",
    "        frequency, gain, phase = load_s1p_file(file_path)\n",
    "        \n",
    "        if gain is None or phase is None:\n",
    "            continue\n",
    "            \n",
    "        # Verify we have 101 values for both gain and phase\n",
    "        if len(gain) != 101 or len(phase) != 101:\n",
    "            print(f\"Skipping {file_path}: Invalid data length\")\n",
    "            continue\n",
    "            \n",
    "        # Concatenate gain and phase as features\n",
    "        features = np.concatenate([gain, phase])\n",
    "        \n",
    "        # Extract metadata\n",
    "        product_type, storage = extract_metadata(file_path)\n",
    "        \n",
    "        # Append to lists\n",
    "        all_features.append(features)\n",
    "        product_types.append(product_type)\n",
    "        storage_conditions.append(storage)\n",
    "        filenames.append(os.path.basename(file_path))\n",
    "\n",
    "    # Convert features to numpy array\n",
    "    X = np.array(all_features)\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = ([f'gain_{i}' for i in range(101)] + \n",
    "                    [f'phase_{i}' for i in range(101)])\n",
    "    \n",
    "    # Create DataFrame with metadata first, then features\n",
    "    df = pd.DataFrame({\n",
    "        'Filename': filenames,\n",
    "        'Product_Type': product_types,\n",
    "        'Storage_Condition': storage_conditions\n",
    "    })\n",
    "    \n",
    "    # Add feature columns\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        df[feature_name] = X[:, i]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Set the path to your data folder\n",
    "    data_folder = './Food/Bakery'  # Adjust this path as needed\n",
    "    \n",
    "    try:\n",
    "        # Create the dataset\n",
    "        print(\"Processing files...\")\n",
    "        df = create_dataset(data_folder)\n",
    "        \n",
    "        # Save the processed dataset\n",
    "        output_file = 'processed_bakery_data.csv'\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\nDataset created successfully!\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"\\nNumber of samples processed: {len(df)}\")\n",
    "        print(f\"Number of features: {len(df.columns) - 3}\")  # Subtract 3 for metadata columns\n",
    "        \n",
    "        # Print some basic information\n",
    "        print(\"\\nProduct Type distribution:\")\n",
    "        print(df['Product_Type'].value_counts())\n",
    "        print(\"\\nStorage Condition distribution:\")\n",
    "        print(df['Storage_Condition'].value_counts())\n",
    "        \n",
    "        # Verify feature dimensions\n",
    "        print(\"\\nFeature verification:\")\n",
    "        print(f\"Number of gain features: {len([col for col in df.columns if col.startswith('gain')])}\")\n",
    "        print(f\"Number of phase features: {len([col for col in df.columns if col.startswith('phase')])}\")\n",
    "        \n",
    "        # Display first few columns to verify order\n",
    "        print(\"\\nFirst few columns:\")\n",
    "        print(df.columns[:10].tolist())\n",
    "        \n",
    "        # Save a sample of the data (first 5 rows) to a separate file for verification\n",
    "        df.head().to_csv('sample_processed_data.csv', index=False)\n",
    "        print(\"\\nSample data saved to 'sample_processed_data.csv' for verification\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataset: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data\n",
    "data = pd.read_csv('processed_bakery_data.csv')\n",
    "\n",
    "# First few columns should be metadata\n",
    "print(\"First columns:\", data.columns[:5])\n",
    "\n",
    "# Verify the order\n",
    "assert data.columns[0] == 'Filename'\n",
    "assert data.columns[1] == 'Product_Type'\n",
    "assert data.columns[2] == 'Storage_Condition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original data...\n",
      "Generating low-noise samples...\n",
      "Generating high-noise samples...\n",
      "Combining datasets...\n",
      "Saving augmented dataset...\n",
      "\n",
      "Augmentation Summary:\n",
      "Original samples: 60\n",
      "Total augmented samples: 180\n",
      "\n",
      "Product Type distribution:\n",
      "Product_Type\n",
      "Bread      90\n",
      "Cookies    90\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Storage Condition distribution:\n",
      "Storage_Condition\n",
      "Open       60\n",
      "Wrapped    60\n",
      "Humid      60\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Gain Values Statistics:\n",
      "Original data mean: 0.986492\n",
      "Low noise data mean: 0.986480\n",
      "High noise data mean: 0.986158\n",
      "\n",
      "Augmented dataset saved to augmented_bakery_data.csv\n",
      "Signal comparison plot saved to 'signal_comparison.png'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_gaussian_noise(data, noise_level):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to gain values while preserving phases\n",
    "    \n",
    "    Parameters:\n",
    "    data: DataFrame containing the original data\n",
    "    noise_level: Standard deviation of the Gaussian noise\n",
    "    \"\"\"\n",
    "    # Create a copy of the original data\n",
    "    noisy_data = data.copy()\n",
    "    \n",
    "    # Get gain column names\n",
    "    gain_columns = [col for col in data.columns if col.startswith('gain')]\n",
    "    \n",
    "    # Add noise only to gain values\n",
    "    for col in gain_columns:\n",
    "        original_values = noisy_data[col].values\n",
    "        noise = np.random.normal(0, noise_level, size=len(original_values))\n",
    "        noisy_data[col] = original_values + noise\n",
    "        \n",
    "        # Ensure gains stay in reasonable range (0 to 1)\n",
    "        noisy_data[col] = np.clip(noisy_data[col], 0, 1)\n",
    "    \n",
    "    return noisy_data\n",
    "\n",
    "def augment_dataset(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Augment the dataset by adding noise-based samples\n",
    "    \"\"\"\n",
    "    # Load the original data\n",
    "    print(\"Loading original data...\")\n",
    "    original_data = pd.read_csv(input_file)\n",
    "    \n",
    "    # Define noise levels\n",
    "    low_noise_level = 0.001   # Low noise (0.1% of signal)\n",
    "    high_noise_level = 0.005  # Higher noise (0.5% of signal)\n",
    "    \n",
    "    # Generate noisy versions\n",
    "    print(\"Generating low-noise samples...\")\n",
    "    low_noise_data = add_gaussian_noise(original_data, low_noise_level)\n",
    "    print(\"Generating high-noise samples...\")\n",
    "    high_noise_data = add_gaussian_noise(original_data, high_noise_level)\n",
    "    \n",
    "    # Add noise level indicator to filenames\n",
    "    low_noise_data['Filename'] = low_noise_data['Filename'].apply(lambda x: f\"low_noise_{x}\")\n",
    "    high_noise_data['Filename'] = high_noise_data['Filename'].apply(lambda x: f\"high_noise_{x}\")\n",
    "    \n",
    "    # Concatenate all datasets\n",
    "    print(\"Combining datasets...\")\n",
    "    augmented_data = pd.concat([original_data, low_noise_data, high_noise_data], \n",
    "                              axis=0, ignore_index=True)\n",
    "    \n",
    "    # Save augmented dataset\n",
    "    print(\"Saving augmented dataset...\")\n",
    "    augmented_data.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nAugmentation Summary:\")\n",
    "    print(f\"Original samples: {len(original_data)}\")\n",
    "    print(f\"Total augmented samples: {len(augmented_data)}\")\n",
    "    print(\"\\nProduct Type distribution:\")\n",
    "    print(augmented_data['Product_Type'].value_counts())\n",
    "    print(\"\\nStorage Condition distribution:\")\n",
    "    print(augmented_data['Storage_Condition'].value_counts())\n",
    "    \n",
    "    # Verify noise addition\n",
    "    gain_columns = [col for col in augmented_data.columns if col.startswith('gain')]\n",
    "    original_stats = original_data[gain_columns].mean().mean()\n",
    "    low_noise_stats = low_noise_data[gain_columns].mean().mean()\n",
    "    high_noise_stats = high_noise_data[gain_columns].mean().mean()\n",
    "    \n",
    "    print(\"\\nGain Values Statistics:\")\n",
    "    print(f\"Original data mean: {original_stats:.6f}\")\n",
    "    print(f\"Low noise data mean: {low_noise_stats:.6f}\")\n",
    "    print(f\"High noise data mean: {high_noise_stats:.6f}\")\n",
    "    \n",
    "    # Create visualization of original vs noisy signals for verification\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Plot example signals\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sample_idx = 0  # First sample\n",
    "    gain_values = range(101)\n",
    "    \n",
    "    plt.plot(gain_values, original_data.iloc[sample_idx][gain_columns], \n",
    "             label='Original', alpha=0.7)\n",
    "    plt.plot(gain_values, low_noise_data.iloc[sample_idx][gain_columns], \n",
    "             label='Low Noise', alpha=0.7)\n",
    "    plt.plot(gain_values, high_noise_data.iloc[sample_idx][gain_columns], \n",
    "             label='High Noise', alpha=0.7)\n",
    "    \n",
    "    plt.title('Comparison of Original and Noisy Signals')\n",
    "    plt.xlabel('Frequency Index')\n",
    "    plt.ylabel('Gain Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('signal_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "def main():\n",
    "    input_file = 'processed_bakery_data.csv'\n",
    "    output_file = 'augmented_bakery_data.csv'\n",
    "    \n",
    "    try:\n",
    "        augmented_data = augment_dataset(input_file, output_file)\n",
    "        print(f\"\\nAugmented dataset saved to {output_file}\")\n",
    "        print(\"Signal comparison plot saved to 'signal_comparison.png'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during data augmentation: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
