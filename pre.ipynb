{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# ðŸŒŸ BAKERY-Spectra-Classification Project\n",
    "## Data Visualization and Processing\n",
    "\n",
    "This notebook contains three main components:\n",
    "1. ðŸ“Š Storage Conditions Visualization\n",
    "2. ðŸ“ˆ Data Processing and Feature Extraction\n",
    "3. ðŸ”„ Data Augmentation\n",
    "\n",
    "### Dataset Information\n",
    "- **Products**: Bread (A) and Cookies (B)\n",
    "- **Storage Conditions**: Open (1), Wrapped (2), Humid (3)\n",
    "- **Measurements**: Frequency (300-900 MHz), Gain, Phase\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## ðŸ“Š Part 1: Storage Conditions Visualization\n",
    "Visualize frequency vs. gain for different storage conditions and product types\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage Conditions Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_s1p_data(file_path):\n",
    "    \"\"\"Load data from s1p file\"\"\"\n",
    "    data = np.loadtxt(file_path, skiprows=9)  # Skip header rows\n",
    "    freq = data[:, 0]\n",
    "    gain = data[:, 1]\n",
    "    return freq, gain\n",
    "\n",
    "def extract_metadata(filename):\n",
    "    \"\"\"Extract product type (bread/cookies) and storage condition from filename\"\"\"\n",
    "    # Format: A/B_1/2/3_1.s1p where:\n",
    "    # A = bread, B = cookies\n",
    "    # 1/2/3 = storage (1:Open, 2:Wrapped, 3:Humid)\n",
    "    # Last number = sample number\n",
    "    base = os.path.basename(filename)\n",
    "    product_code = base[0]  # A or B\n",
    "    product_type = 'Bread' if product_code == 'A' else 'Cookies'\n",
    "    storage_num = int(base.split('_')[1])\n",
    "    \n",
    "    storage_map = {1: 'Open', 2: 'Wrapped', 3: 'Humid'}\n",
    "    storage = storage_map[storage_num]\n",
    "    \n",
    "    return product_code, product_type, storage\n",
    "\n",
    "def plot_storage_conditions(product_code=None):\n",
    "    \"\"\"Create separate plots for each storage condition for specified product type\"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    title_suffix = {\n",
    "        'A': 'Bread',\n",
    "        'B': 'Cookies',\n",
    "        None: 'Bread and Cookies'\n",
    "    }[product_code]\n",
    "    \n",
    "    fig.suptitle(f'Frequency vs. Gain for Different Storage Conditions - {title_suffix}', \n",
    "                fontsize=16)\n",
    "    \n",
    "    pattern = (f'./RawData/Bakery/{product_code}_*_*.s1p' if product_code \n",
    "              else './RawData/Bakery/[A-B]_*_*.s1p')\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    storage_data = {'Open': [], 'Wrapped': [], 'Humid': []}\n",
    "    \n",
    "    for file_path in files:\n",
    "        freq, gain = load_s1p_data(file_path)\n",
    "        prod_code, prod_type, storage = extract_metadata(file_path)\n",
    "        storage_data[storage].append((freq, gain, prod_type))\n",
    "    \n",
    "    color_map = {'Bread': 'blue', 'Cookies': 'purple'}\n",
    "    \n",
    "    for ax, (title, data) in zip([ax1, ax2, ax3], \n",
    "                               [('Open Storage', storage_data['Open']),\n",
    "                                ('Wrapped Storage', storage_data['Wrapped']),\n",
    "                                ('Humid Storage', storage_data['Humid'])]):\n",
    "        ax.set_title(title)\n",
    "        for freq, gain, prod_type in data:\n",
    "            ax.plot(freq, gain, alpha=0.3, color=color_map[prod_type], \n",
    "                   label=prod_type)\n",
    "        ax.set_xlabel('Frequency (Hz)')\n",
    "        ax.set_ylabel('Gain')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    if not product_code:\n",
    "        handles, labels = ax1.get_legend_handles_labels()\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        ax1.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    product_name = {\n",
    "        'A': 'bread',\n",
    "        'B': 'cookies',\n",
    "        None: 'bread_and_cookies'\n",
    "    }[product_code]\n",
    "    filename = f'storage_conditions_comparison_{product_name}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_storage_conditions('A')  # For Bread samples\n",
    "plot_storage_conditions('B')  # For Cookie samples\n",
    "plot_storage_conditions()     # For both Bread and Cookies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## ðŸ“ˆ Part 2: Data Processing and Feature Extraction\n",
    "Process raw data files and create structured dataset with features\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: [Code] - Imports and Utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob as glob_pattern  # Rename glob to avoid conflict\n",
    "\n",
    "def load_s1p_file(file_path):\n",
    "    \"\"\"\n",
    "    Load an s1p file and return frequency, gain, and phase data\n",
    "    Skip the metadata row (first row)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Skip the first row (metadata) and load the data\n",
    "        data = pd.read_csv(file_path, delimiter=r'\\s+', header=None, skiprows=1)\n",
    "        \n",
    "        # Verify we have 101 rows of data\n",
    "        if len(data) != 101:\n",
    "            print(f\"Warning: {file_path} has {len(data)} rows instead of 101\")\n",
    "            \n",
    "        frequency = data.iloc[:, 0]\n",
    "        gain = data.iloc[:, 1]\n",
    "        phase = data.iloc[:, 2]\n",
    "        \n",
    "        return frequency, gain, phase\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "def extract_metadata(filename):\n",
    "    \"\"\"\n",
    "    Extract product type and storage condition from filename\n",
    "    Example: A_1_1.s1p -> Product Type: A (bread), Storage: 1 (open)\n",
    "    \"\"\"\n",
    "    parts = os.path.basename(filename).split('_')\n",
    "    product_type = 'Bread' if parts[0] == 'A' else 'Cookies'\n",
    "    storage_condition = {\n",
    "        '1': 'Open',\n",
    "        '2': 'Wrapped',\n",
    "        '3': 'Humid'\n",
    "    }.get(parts[1], 'Unknown')\n",
    "    \n",
    "    return product_type, storage_condition\n",
    "\n",
    "def create_dataset(data_folder):\n",
    "    \"\"\"\n",
    "    Create a dataset from all s1p files in the folder\n",
    "    \"\"\"\n",
    "    # Get all s1p files\n",
    "    s1p_files = glob_pattern(os.path.join(data_folder, '*.s1p'))\n",
    "    s1p_files.sort()  # Sort files to ensure consistent ordering\n",
    "    \n",
    "    if not s1p_files:\n",
    "        raise ValueError(f\"No .s1p files found in {data_folder}\")\n",
    "\n",
    "    # Lists to store data\n",
    "    all_features = []\n",
    "    metadata = []\n",
    "\n",
    "    # Process each file\n",
    "    for file_path in s1p_files:\n",
    "        # Load data\n",
    "        frequency, gain, phase = load_s1p_file(file_path)\n",
    "        \n",
    "        if gain is None or phase is None:\n",
    "            continue\n",
    "            \n",
    "        # Verify we have 101 values for both gain and phase\n",
    "        if len(gain) != 101 or len(phase) != 101:\n",
    "            print(f\"Skipping {file_path}: Invalid data length\")\n",
    "            continue\n",
    "            \n",
    "        # Concatenate gain and phase as features\n",
    "        features = np.concatenate([gain, phase])\n",
    "        \n",
    "        # Extract metadata\n",
    "        product_type, storage = extract_metadata(file_path)\n",
    "        \n",
    "        # Append to lists\n",
    "        all_features.append(features)\n",
    "        metadata.append({\n",
    "            'Filename': os.path.basename(file_path),\n",
    "            'Product_Type': product_type,\n",
    "            'Storage_Condition': storage\n",
    "        })\n",
    "\n",
    "    # Convert features to numpy array\n",
    "    X = np.array(all_features)\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = ([f'gain_{i}' for i in range(101)] + \n",
    "                    [f'phase_{i}' for i in range(101)])\n",
    "    \n",
    "    # Create feature dictionary\n",
    "    feature_dict = {name: X[:, i] for i, name in enumerate(feature_names)}\n",
    "    \n",
    "    # Create metadata DataFrame\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    \n",
    "    # Create features DataFrame\n",
    "    features_df = pd.DataFrame(feature_dict)\n",
    "    \n",
    "    # Combine metadata and features\n",
    "    df = pd.concat([metadata_df, features_df], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Cell 3: [Code] - Main Execution\n",
    "def main():\n",
    "    # Set the path to your data folder\n",
    "    data_folder = './RawData/Bakery'  # Adjust this path as needed\n",
    "    \n",
    "    try:\n",
    "        # Create the dataset\n",
    "        print(\"Processing files...\")\n",
    "        df = create_dataset(data_folder)\n",
    "        \n",
    "        # Save the processed dataset\n",
    "        output_file = 'processed_bakery_data.csv'\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\nDataset created successfully!\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"\\nNumber of samples processed: {len(df)}\")\n",
    "        print(f\"Number of features: {len(df.columns) - 3}\")  # Subtract 3 for metadata columns\n",
    "        \n",
    "        # Print some basic information\n",
    "        print(\"\\nProduct Type distribution:\")\n",
    "        print(df['Product_Type'].value_counts())\n",
    "        print(\"\\nStorage Condition distribution:\")\n",
    "        print(df['Storage_Condition'].value_counts())\n",
    "        \n",
    "        # Verify feature dimensions\n",
    "        print(\"\\nFeature verification:\")\n",
    "        print(f\"Number of gain features: {len([col for col in df.columns if col.startswith('gain')])}\")\n",
    "        print(f\"Number of phase features: {len([col for col in df.columns if col.startswith('phase')])}\")\n",
    "        \n",
    "        # Display first few columns to verify order\n",
    "        print(\"\\nFirst few columns:\")\n",
    "        print(df.columns[:10].tolist())\n",
    "        \n",
    "        # Save a sample of the data (first 5 rows) to a separate file for verification\n",
    "        df.head().to_csv('sample_processed_data.csv', index=False)\n",
    "        print(\"\\nSample data saved to 'sample_processed_data.csv' for verification\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataset: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First columns: Index(['Filename', 'Product_Type', 'Storage_Condition', 'gain_0', 'gain_1'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the processed data\n",
    "data = pd.read_csv('processed_bakery_data.csv')\n",
    "\n",
    "# First few columns should be metadata\n",
    "print(\"First columns:\", data.columns[:5])\n",
    "\n",
    "# Verify the order\n",
    "assert data.columns[0] == 'Filename'\n",
    "assert data.columns[1] == 'Product_Type'\n",
    "assert data.columns[2] == 'Storage_Condition'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## ðŸ”„ Part 3: Data Augmentation\n",
    "Augment the dataset by adding noise variations\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_gaussian_noise(data, noise_level):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to gain values while preserving phases\n",
    "    \n",
    "    Parameters:\n",
    "    data: DataFrame containing the original data\n",
    "    noise_level: Standard deviation of the Gaussian noise\n",
    "    \"\"\"\n",
    "    # Create a copy of the original data\n",
    "    noisy_data = data.copy()\n",
    "    \n",
    "    # Get gain column names\n",
    "    gain_columns = [col for col in data.columns if col.startswith('gain')]\n",
    "    \n",
    "    # Add noise only to gain values\n",
    "    for col in gain_columns:\n",
    "        original_values = noisy_data[col].values\n",
    "        noise = np.random.normal(0, noise_level, size=len(original_values))\n",
    "        noisy_data[col] = original_values + noise\n",
    "        \n",
    "        # Ensure gains stay in reasonable range (0 to 1)\n",
    "        noisy_data[col] = np.clip(noisy_data[col], 0, 1)\n",
    "    \n",
    "    return noisy_data\n",
    "\n",
    "def augment_dataset(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Augment the dataset by adding noise-based samples\n",
    "    \"\"\"\n",
    "    # Load the original data\n",
    "    print(\"Loading original data...\")\n",
    "    original_data = pd.read_csv(input_file)\n",
    "    \n",
    "    # Define noise levels\n",
    "    low_noise_level = 0.001   # Low noise (0.1% of signal)\n",
    "    high_noise_level = 0.005  # Higher noise (0.5% of signal)\n",
    "    \n",
    "    # Generate noisy versions\n",
    "    print(\"Generating low-noise samples...\")\n",
    "    low_noise_data = add_gaussian_noise(original_data, low_noise_level)\n",
    "    print(\"Generating high-noise samples...\")\n",
    "    high_noise_data = add_gaussian_noise(original_data, high_noise_level)\n",
    "    \n",
    "    # Add noise level indicator to filenames\n",
    "    low_noise_data['Filename'] = low_noise_data['Filename'].apply(lambda x: f\"low_noise_{x}\")\n",
    "    high_noise_data['Filename'] = high_noise_data['Filename'].apply(lambda x: f\"high_noise_{x}\")\n",
    "    \n",
    "    # Concatenate all datasets\n",
    "    print(\"Combining datasets...\")\n",
    "    augmented_data = pd.concat([original_data, low_noise_data, high_noise_data], \n",
    "                              axis=0, ignore_index=True)\n",
    "    \n",
    "    # Save augmented dataset\n",
    "    print(\"Saving augmented dataset...\")\n",
    "    augmented_data.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nAugmentation Summary:\")\n",
    "    print(f\"Original samples: {len(original_data)}\")\n",
    "    print(f\"Total augmented samples: {len(augmented_data)}\")\n",
    "    \n",
    "    print(\"\\nProduct Type distribution:\")\n",
    "    print(augmented_data['Product_Type'].value_counts())\n",
    "    \n",
    "    print(\"\\nStorage Condition distribution:\")\n",
    "    print(augmented_data['Storage_Condition'].value_counts())\n",
    "    \n",
    "    print(\"\\nGain Values Statistics:\")\n",
    "    gain_columns = [col for col in original_data.columns if col.startswith('gain')]\n",
    "    print(f\"Original data mean: {original_data[gain_columns].mean().mean():.6f}\")\n",
    "    print(f\"Low noise data mean: {low_noise_data[gain_columns].mean().mean():.6f}\")\n",
    "    print(f\"High noise data mean: {high_noise_data[gain_columns].mean().mean():.6f}\")\n",
    "\n",
    "    return original_data, low_noise_data, high_noise_data, augmented_data\n",
    "\n",
    "# Define the main function\n",
    "# Execute augmentation\n",
    "input_file = 'processed_bakery_data.csv'\n",
    "output_file = 'augmented_bakery_data.csv'\n",
    "\n",
    "try:\n",
    "    original_data, low_noise_data, high_noise_data, augmented_data = augment_dataset(input_file, output_file)\n",
    "    print(f\"\\nAugmented dataset saved to {output_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during data augmentation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_augmentation_results(original_data, low_noise_data, high_noise_data):\n",
    "    \"\"\"\n",
    "    Create two plots:\n",
    "    1. Signal comparison between original and noisy versions\n",
    "    2. Distribution of original data\n",
    "    \"\"\"\n",
    "    # Get gain columns\n",
    "    gain_columns = [col for col in original_data.columns if col.startswith('gain')]\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    plt.figure(figsize=(15, 5))    \n",
    "    # 1. Signal Comparison Plot\n",
    "    sample_idx = 0  # First sample\n",
    "    gain_values = range(101)\n",
    "    \n",
    "    plt.plot(gain_values, original_data.iloc[sample_idx][gain_columns], \n",
    "             label='Original', alpha=0.7)\n",
    "    plt.plot(gain_values, low_noise_data.iloc[sample_idx][gain_columns], \n",
    "             label='Low Noise', alpha=0.7)\n",
    "    plt.plot(gain_values, high_noise_data.iloc[sample_idx][gain_columns], \n",
    "             label='High Noise', alpha=0.7)\n",
    "    \n",
    "    plt.title('Comparison of Original and Noisy Signals')\n",
    "    plt.xlabel('Frequency Index')\n",
    "    plt.ylabel('Gain Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('signal_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    # 2. Original Data Distribution\n",
    "    sns.histplot(data=original_data[gain_columns].values.ravel(), \n",
    "                color='blue', alpha=0.7)\n",
    "    plt.title('Distribution of Original Data')\n",
    "    plt.xlabel('Gain Value')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('augmentation_analysis.png')\n",
    "    plt.show()\n",
    "    \n",
    "# Define the main function\n",
    "def main():\n",
    "    # Execute visualization\n",
    "    try:\n",
    "        visualize_augmentation_results(original_data, low_noise_data, high_noise_data)\n",
    "        print(\"Visualization saved as 'augmentation_analysis.png'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during visualization: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_data_distribution(data_path):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the distribution of the augmented dataset\n",
    "    \"\"\"\n",
    "    # Load the augmented data\n",
    "    print(\"Loading augmented data...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Create a directory for plots if it doesn't exist\n",
    "    os.makedirs('distribution_plots', exist_ok=True)\n",
    "    \n",
    "    # 1. Distribution of samples across categories\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(data=df, x='Product_Type', hue='Storage_Condition')\n",
    "    plt.title('Distribution of Samples across Categories')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribution_plots/category_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Box plots for gain and phase features\n",
    "    gain_cols = [col for col in df.columns if col.startswith('gain')]\n",
    "    phase_cols = [col for col in df.columns if col.startswith('phase')]\n",
    "    \n",
    "    # Calculate statistics for gain features\n",
    "    gain_stats = df[gain_cols].agg(['mean', 'std', 'min', 'max'])\n",
    "    print(\"\\nGain Features Statistics:\")\n",
    "    print(gain_stats.describe())\n",
    "    \n",
    "    # Box plot for gain values by product type\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df_melted_gain = df.melt(id_vars=['Product_Type'], value_vars=gain_cols)\n",
    "    sns.boxplot(data=df_melted_gain, x='Product_Type', y='value')\n",
    "    plt.title('Distribution of Gain Values by Product Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Gain Values')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribution_plots/gain_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Check for outliers using IQR method\n",
    "    def detect_outliers(data):\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).sum()\n",
    "        return outliers\n",
    "    \n",
    "    print(\"\\nNumber of outliers in gain features:\")\n",
    "    outliers_gain = {col: detect_outliers(df[col]) for col in gain_cols}\n",
    "    print(pd.Series(outliers_gain).describe())\n",
    "    \n",
    "    # 4. Correlation heatmap (sample of features)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Select a subset of features to make the heatmap readable\n",
    "    selected_features = gain_cols[:10] + phase_cols[:10]\n",
    "    correlation_matrix = df[selected_features].corr()\n",
    "    sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, annot=False)\n",
    "    plt.title('Correlation Heatmap (First 10 Gain and Phase Features)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribution_plots/correlation_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Distribution of features by noise level\n",
    "    def get_noise_level(filename):\n",
    "        if 'high_noise' in filename:\n",
    "            return 'High Noise'\n",
    "        elif 'low_noise' in filename:\n",
    "            return 'Low Noise'\n",
    "        return 'Original'\n",
    "    \n",
    "    df['Noise_Level'] = df['Filename'].apply(get_noise_level)\n",
    "    \n",
    "    # Plot distribution of a sample gain feature across noise levels\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sample_gain = gain_cols[50]  # Middle frequency\n",
    "    sns.kdeplot(data=df, x=sample_gain, hue='Noise_Level')\n",
    "    plt.title(f'Distribution of {sample_gain} across Noise Levels')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribution_plots/noise_level_distribution.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the analysis\n",
    "df = analyze_data_distribution('augmented_bakery_data.csv')\n",
    "\n",
    "print(\"\\nAnalysis complete! Check the 'distribution_plots' directory for visualizations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
