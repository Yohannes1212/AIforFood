{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import json\n",
    "from itertools import cycle\n",
    "\n",
    "# Import models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Constants\n",
    "RANDOM_STATE = 42\n",
    "CV_SPLITS = 5\n",
    "\n",
    "RESULTS_DIR = 'results/Product_Classification_results'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Cell 2: Load Best Parameters and Create Models\n",
    "def load_best_parameters():\n",
    "    \"\"\"Load best parameters from JSON file\"\"\"\n",
    "    with open('hyperparameter_tuning/product_type_results.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_models(best_params, scenario='with_storage_condition'):\n",
    "    \"\"\"Create models with best parameters\"\"\"\n",
    "    models = {}\n",
    "    params = best_params[scenario]\n",
    "    \n",
    "    for model_name, model_params in params.items():\n",
    "        if model_name == 'SVM':\n",
    "            models[model_name] = SVC(**model_params, random_state= RANDOM_STATE, probability=True)\n",
    "        elif model_name == 'Random Forest':\n",
    "            models[model_name] = RandomForestClassifier(**model_params, random_state= RANDOM_STATE)\n",
    "        elif model_name == 'KNN':\n",
    "            models[model_name] = KNeighborsClassifier(**model_params)\n",
    "        elif model_name == 'Neural Network':\n",
    "            models[model_name] = MLPClassifier(**model_params, random_state= RANDOM_STATE)\n",
    "        elif model_name == 'Logistic Regression':\n",
    "            models[model_name] = LogisticRegression(**model_params, random_state= RANDOM_STATE)\n",
    "    \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "def plot_model_comparison(results_without, results_with):\n",
    "    \"\"\"Plot model comparison bar chart\"\"\"\n",
    "    models = list(results_without.keys())\n",
    "    \n",
    "    means_without = [results_without[model]['mean_accuracy'] for model in models]\n",
    "    stds_without = [results_without[model]['std_accuracy'] for model in models]\n",
    "    \n",
    "    means_with = [results_with[model]['mean_accuracy'] for model in models]\n",
    "    stds_with = [results_with[model]['std_accuracy'] for model in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    rects1 = ax.bar(x - width/2, means_without, width, yerr=stds_without,\n",
    "                    label='Without Storage Condition', capsize=5)\n",
    "    rects2 = ax.bar(x + width/2, means_with, width, yerr=stds_with,\n",
    "                    label='With Storage Condition', capsize=5)\n",
    "    \n",
    "    ax.set_ylabel('Mean Accuracy')\n",
    "    ax.set_title('Model Comparison for Product Type Classification')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate(f'{height:.3f}',\n",
    "                       xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom')\n",
    "    \n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR,'product_type_comparison.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(results_without, results_with, class_labels):\n",
    "    \"\"\"Plot confusion matrices side by side for with/without storage condition\"\"\"\n",
    "    for model_name in results_without.keys():\n",
    "        # Create two subplots side by side\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot first confusion matrix (Without Product Type)\n",
    "        sns.heatmap(results_without[model_name]['confusion_matrix'], \n",
    "                   annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=range(len(class_labels)),\n",
    "                   yticklabels=range(len(class_labels)), ax=ax1)\n",
    "        ax1.set_title(f'{model_name} - Without Storage Condition')\n",
    "        ax1.set_ylabel('Actual')\n",
    "        ax1.set_xlabel('Predicted')\n",
    "        \n",
    "        # Plot second confusion matrix (With Product Type)\n",
    "        sns.heatmap(results_with[model_name]['confusion_matrix'], \n",
    "                   annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=range(len(class_labels)),\n",
    "                   yticklabels=range(len(class_labels)), ax=ax2)\n",
    "        ax2.set_title(f'{model_name} - With Storage Condition')\n",
    "        ax2.set_ylabel('Actual')\n",
    "        ax2.set_xlabel('Predicted')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, filename))\n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def plot_learning_curves(X, y, model, model_name, cv=CV_SPLITS):\n",
    "    \"\"\"Plot learning curves with confidence intervals\"\"\"\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    #cv = StratifiedKFold(n_splits=CV_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y,\n",
    "        train_sizes=train_sizes,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, label='Training Score', color='blue', marker='o')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n",
    "                     alpha=0.15, color='blue')\n",
    "    plt.plot(train_sizes, val_mean, label='Cross-validation Score', color='red', marker='o')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, \n",
    "                     alpha=0.15, color='red')\n",
    "    \n",
    "    plt.title(f'Learning Curves - {model_name}')\n",
    "    plt.xlabel('Training Examples')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    final_train = f\"Final training score: {train_mean[-1]:.4f} ± {train_std[-1]:.4f}\"\n",
    "    final_val = f\"Final validation score: {val_mean[-1]:.4f} ± {val_std[-1]:.4f}\"\n",
    "    plt.annotate(final_train, xy=(0.6, 0.2), xycoords='axes fraction')\n",
    "    plt.annotate(final_val, xy=(0.6, 0.15), xycoords='axes fraction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'learning_curve_{model_name.lower().replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, filename))\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'train_sizes': train_sizes,\n",
    "        'train_scores': {'mean': train_mean, 'std': train_std},\n",
    "        'val_scores': {'mean': val_mean, 'std': val_std}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_statistical_analysis(results_without, results_with):\n",
    "    \"\"\"Perform statistical analysis\"\"\"\n",
    "    analysis_results = []\n",
    "    \n",
    "    print(\"\\nStatistical Analysis Results:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model in results_without.keys():\n",
    "        acc_without = results_without[model]['fold_accuracies']\n",
    "        acc_with = results_with[model]['fold_accuracies']\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_rel(acc_with, acc_without)\n",
    "        improvement = (np.mean(acc_with) - np.mean(acc_without)) * 100\n",
    "        \n",
    "        result = {\n",
    "            'Model': model,\n",
    "            'Accuracy without Storage Condition': f\"{np.mean(acc_without):.4f} ± {np.std(acc_without):.4f}\",\n",
    "            'Accuracy with Storage Condition': f\"{np.mean(acc_with):.4f} ± {np.std(acc_with):.4f}\",\n",
    "            'Improvement (%)': f\"{improvement:.2f}%\",\n",
    "            'p-value': f\"{p_value:.4f}\",\n",
    "            'Significant': \"Yes\" if p_value < 0.05 else \"No\"\n",
    "        }\n",
    "        analysis_results.append(result)\n",
    "        \n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"- Without Storage Condition: {result['Accuracy without Storage Condition']}\")\n",
    "        print(f\"- With Storage Condition: {result['Accuracy with Storage Condition']}\")\n",
    "        print(f\"- Improvement: {result['Improvement (%)']}\")\n",
    "        print(f\"- Statistical Significance (p < 0.05): {result['Significant']} (p = {result['p-value']})\")\n",
    "\n",
    "    results_df = pd.DataFrame(analysis_results)\n",
    "    results_df.to_csv('Product_type_statistical_analysis_results.csv', index=False)\n",
    "    return results_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "def prepare_data():\n",
    "    \"\"\"Load and prepare data consistently\"\"\"\n",
    "    print(\"Loading data and preparing features...\")\n",
    "    data = pd.read_csv('augmented_bakery_data.csv')\n",
    "    \n",
    "    # Encode labels\n",
    "    le_product = LabelEncoder()\n",
    "    le_storage = LabelEncoder()\n",
    "    data['Product_Type_encoded'] = le_product.fit_transform(data['Product_Type'])\n",
    "    data['Storage_Condition_encoded'] = le_storage.fit_transform(data['Storage_Condition'])\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_cols = [col for col in data.columns if col.startswith(('gain_', 'phase_'))]\n",
    "    X_base = data[feature_cols]\n",
    "    \n",
    "    # Scale features on entire dataset\n",
    "    scaler = StandardScaler()\n",
    "    X_base_scaled = scaler.fit_transform(X_base)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    X_without_storage = X_base_scaled\n",
    "    X_with_storage = np.column_stack([X_base_scaled, data['Storage_Condition_encoded'].values.reshape(-1, 1)])\n",
    "    \n",
    "    #target\n",
    "    y_product = data['Product_Type_encoded'].values\n",
    "    \n",
    "    return X_with_storage, X_without_storage, y_product, le_product.classes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "def evaluate_models(X, y, models, cv=CV_SPLITS):\n",
    "    \"\"\"Evaluate models using cross-validation\"\"\"\n",
    "    results = {}\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        fold_accuracies = []\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        \n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "            # Split data (already scaled)\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # Fit and predict\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            fold_accuracies.append(acc)\n",
    "            all_predictions.extend(y_pred)\n",
    "            all_true_labels.extend(y_test)\n",
    "            \n",
    "            print(f\"Fold {fold_idx + 1} accuracy: {acc:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'fold_accuracies': np.array(fold_accuracies),\n",
    "            'mean_accuracy': np.mean(fold_accuracies),\n",
    "            'std_accuracy': np.std(fold_accuracies),\n",
    "            'confusion_matrix': confusion_matrix(all_true_labels, all_predictions),\n",
    "            'classification_report': classification_report(all_true_labels, all_predictions)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{model_name} Final Results:\")\n",
    "        print(f\"Mean accuracy: {results[model_name]['mean_accuracy']:.4f} ± {results[model_name]['std_accuracy']:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(results[model_name]['classification_report'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_summary(results_without, results_with, stats_results):\n",
    "    \"\"\"Generate and save comprehensive analysis summary\"\"\"\n",
    "    \n",
    "    with open('Product_type_analysis_summary.txt', 'w') as f:\n",
    "        # Header\n",
    "        f.write(\"Product Type Classification Analysis Summary\\n\")\n",
    "        f.write(\"==================================================\\n\\n\")\n",
    "        \n",
    "        # 1. Best Performing Models section\n",
    "        f.write(\"1. Best Performing Models:\\n\\n\")\n",
    "        \n",
    "        # Find best models\n",
    "        best_without = max(results_without.items(), \n",
    "                         key=lambda x: x[1]['mean_accuracy'])\n",
    "        best_with = max(results_with.items(), \n",
    "                       key=lambda x: x[1]['mean_accuracy'])\n",
    "        \n",
    "        # Without Storage Condition \n",
    "        f.write(\"Without Storage Condition:\\n\")\n",
    "        f.write(f\"- Best Model: {best_without[0]}\\n\")\n",
    "        f.write(f\"- Accuracy: {best_without[1]['mean_accuracy']:.4f} ± \"\n",
    "                f\"{best_without[1]['std_accuracy']:.4f}\\n\\n\")\n",
    "        \n",
    "        # With Storage Condition\n",
    "        f.write(\"With Storage Condition:\\n\")\n",
    "        f.write(f\"- Best Model: {best_with[0]}\\n\")\n",
    "        f.write(f\"- Accuracy: {best_with[1]['mean_accuracy']:.4f} ± \"\n",
    "                f\"{best_with[1]['std_accuracy']:.4f}\\n\\n\")\n",
    "        \n",
    "        # 2. Impact of Storage Condition section\n",
    "        f.write(\"2. Impact of Storage Condition:\\n\")\n",
    "        \n",
    "        # Create DataFrame for formatted table\n",
    "        data = []\n",
    "        for model_name in results_without.keys():\n",
    "            acc_without = results_without[model_name]['mean_accuracy']\n",
    "            std_without = results_without[model_name]['std_accuracy']\n",
    "            \n",
    "            acc_with = results_with[model_name]['mean_accuracy']\n",
    "            std_with = results_with[model_name]['std_accuracy']\n",
    "            \n",
    "            improvement = ((acc_with - acc_without) / acc_without) * 100\n",
    "            \n",
    "            # Get p-value from stats_results DataFrame\n",
    "            p_value = float(stats_results[stats_results['Model'] == model_name]['p-value'].values[0].strip())\n",
    "            \n",
    "            data.append({\n",
    "                'Model': model_name,\n",
    "                'Accuracy without Storage Condition': f\"{acc_without:.4f} ± {std_without:.4f}\",\n",
    "                'Accuracy with Storage Condition': f\"{acc_with:.4f} ± {std_with:.4f}\",\n",
    "                'Improvement (%)': f\"{improvement:.2f}%\",\n",
    "                'p-value': f\"{p_value:.4f}\",\n",
    "                'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame and write as formatted table\n",
    "        df = pd.DataFrame(data)\n",
    "        f.write(df.to_string(index=False))\n",
    "        \n",
    "        # Additional Analysis\n",
    "        f.write(\"\\n\\n3. Additional Insights:\\n\")\n",
    "        f.write(\"-----------------------\\n\")\n",
    "        \n",
    "        # Count models with significant improvement\n",
    "        significant_improvements = sum(1 for d in data if float(d['Improvement (%)'].strip('%')) > 0 \n",
    "                                    and d['Significant'] == 'Yes')\n",
    "        f.write(f\"\\nNumber of models with significant improvement: {significant_improvements}\\n\")\n",
    "        \n",
    "        # Best improvement\n",
    "        best_improvement = max(data, key=lambda x: float(x['Improvement (%)'].strip('%')))\n",
    "        f.write(f\"Model with best improvement: {best_improvement['Model']} \"\n",
    "                f\"({best_improvement['Improvement (%)']})\\n\")\n",
    "        \n",
    "        # Overall recommendation\n",
    "        f.write(\"\\nOverall Recommendation:\\n\")\n",
    "        if significant_improvements > 0:\n",
    "            f.write(\"Including storage condition information appears beneficial for classification performance.\\n\")\n",
    "        else:\n",
    "            f.write(\"Storage condition information does not significantly improve classification performance.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Prepare Data\n",
    "    X_with_storage, X_without_storage, y_product, class_labels = prepare_data()\n",
    "    \n",
    "    # 2. Load Models\n",
    "    print(\"Loading best parameters and creating models...\")\n",
    "    best_params = load_best_parameters()\n",
    "    models_with_storage = create_models(best_params, 'with_storage_condition')\n",
    "    models_without_storage = create_models(best_params, 'without_storage_condition')\n",
    "    \n",
    "    # 3. Evaluate Models\n",
    "    print(\"\\nEvaluating models with storage condition...\")\n",
    "    results_with_storage = evaluate_models(X_with_storage, y_product, models_with_storage)\n",
    "    \n",
    "    print(\"\\nEvaluating models without storage condition...\")\n",
    "    results_without_storage = evaluate_models(X_without_storage, y_product, models_without_storage)\n",
    "    \n",
    "    # 4. Generate Visualizations and Analysis\n",
    "    print(\"\\nGenerating visualizations and analysis...\")\n",
    "    \n",
    "    # Model comparison plot\n",
    "    plot_model_comparison(results_without_storage, results_with_storage)\n",
    "    \n",
    "    # Confusion matrices\n",
    "    plot_confusion_matrices(results_without_storage, results_with_storage, class_labels)\n",
    "    \n",
    "    # Statistical analysis\n",
    "    stats_results = perform_statistical_analysis(results_without_storage, results_with_storage)\n",
    "    \n",
    "    # Save analysis summary\n",
    "    save_analysis_summary(results_without_storage, results_with_storage, stats_results)\n",
    "\n",
    "    # Learning curves\n",
    "    for model_name, model in models_with_storage.items():\n",
    "        print(f\"\\nGenerating learning curve for {model_name}...\")\n",
    "        plot_learning_curves(X_with_storage, y_product, model, model_name)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
